import os
import streamlit as st
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
import logging
from langchain_community.document_loaders import DirectoryLoader
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnableMap
from langchain_google_genai import ChatGoogleGenerativeAI

VECTOR_STORE_PATH = "data/vector_store.faiss"

# í™˜ê²½ ì„¤ì •
def setup_environment():
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
    os.environ["LANGCHAIN_API_KEY"] = os.getenv("LANGSMITH_API_TOKEN")
    os.environ["LANGCHAIN_PROJECT"] = "SYU-GPT"
    os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
    os.environ["SERPER_API_KEY"] = os.getenv("SERPER_API_KEY")

# ë¬¸ì„œ ì²˜ë¦¬ ì¤€ë¹„
@st.cache_resource
def generate_response(user_input):
    try:
        # íŒŒì¼ë³„ ì„¤ì •
        config = {
            'introduce.txt': {'chunk_size': 1500, 'chunk_overlap': 300},
            'ê´€ë ¨ ë§í¬ data.txt': {'chunk_size': 1500, 'chunk_overlap': 300},
            'êµí†µ data.txt': {'chunk_size': 1500, 'chunk_overlap': 300},
            'ë„ì„œê´€ data.txt': {'chunk_size': 2000, 'chunk_overlap': 300},
            'ë™ì•„ë¦¬ data.txt': {'chunk_size': 4500, 'chunk_overlap': 300},
            'ë“±ë¡ data.txt': {'chunk_size': 2000, 'chunk_overlap': 300},
            'ì„±ì  data.txt': {'chunk_size': 1500, 'chunk_overlap': 300},
            'ì…”í‹€ë²„ìŠ¤ data.txt': {'chunk_size': 1000, 'chunk_overlap': 300},
            'ìˆ˜ê°•ì‹ ì²­ data.txt': {'chunk_size': 1500, 'chunk_overlap': 250},
            'ì‹œì„¤ ì •ë³´ data.txt': {'chunk_size': 2000, 'chunk_overlap': 350},
            'ì—…ë¬´ë³„ ì „í™”ë²ˆí˜¸ data.txt': {'chunk_size': 1000, 'chunk_overlap': 200},
            'ì¥í•™ê¸ˆ data.txt': {'chunk_size': 4000, 'chunk_overlap': 100},
            'ì¡¸ì—… data.txt': {'chunk_size': 1200, 'chunk_overlap': 250},
            'ì¦ëª…ì„œ data.txt': {'chunk_size': 2000, 'chunk_overlap': 250},
            'í•™ê³¼ data.txt': {'chunk_size': 7000, 'chunk_overlap': 500},
            'í•™ì‚¬ ì¼ì • data.txt': {'chunk_size': 1500, 'chunk_overlap': 200},
            'í›„ë¬¸ ì •ë³´ data.txt': {'chunk_size': 2000, 'chunk_overlap': 300},
            'í•™êµ ê±´ë¬¼ data.txt': {'chunk_size': 3000, 'chunk_overlap': 100},
        }

        # HuggingFaceEmbeddings ê°ì²´ ìƒì„±
        model_name = "jhgan/ko-sbert-nli"
        model_kwargs = {'device': 'cpu'}
        encode_kwargs = {'normalize_embeddings': True}
        hf = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs=model_kwargs,
            encode_kwargs=encode_kwargs
        )

        if os.path.exists(VECTOR_STORE_PATH):
            # ì €ì¥ëœ vector storeê°€ ìˆìœ¼ë©´ ì´ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
            docsearch = FAISS.load_local(VECTOR_STORE_PATH, hf, allow_dangerous_deserialization=True)
        else:
            # DirectoryLoaderë¡œ ëª¨ë“  txt íŒŒì¼ ë¡œë“œ
            loader = DirectoryLoader(".", glob="data/SYU_GPT/*.txt", show_progress=True)
            docs = loader.load()
            all_splits = []
            for doc in docs:
                file_path = doc.metadata['source']
                file_name = os.path.basename(file_path)
                if file_name in config:
                    chunk_size = config[file_name]['chunk_size']
                    chunk_overlap = config[file_name]['chunk_overlap']
                else:
                    chunk_size = 1500
                    chunk_overlap = 300
                text_splitter = CharacterTextSplitter(
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap
                )
                splits = text_splitter.split_documents([doc])
                all_splits.extend(splits)
            # ëª¨ë“  ë¶„í• ì´ ì™„ë£Œëœ í›„ì— í•œ ë²ˆë§Œ vectorstoreë¥¼ ìƒì„±
            if all_splits:
                docsearch = FAISS.from_documents(all_splits, hf)
                # ìƒì„±ëœ vector storeë¥¼ ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤.
                docsearch.save_local(VECTOR_STORE_PATH)
            else:
                print("No documents were split or processed.")
                return "ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        retriever = docsearch.as_retriever(
            search_type="mmr",
            search_kwargs={'k': 3, 'fetch_k': 10})
        retriever.get_relevant_documents("í˜ì‹ ì„±ì¥ ì •ì±…ê¸ˆìœµì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì¤˜")
        template = """ë‹¹ì‹ ì˜ ì´ë¦„ì€ SYU-GPTì…ë‹ˆë‹¤. ì‚¼ìœ¡ëŒ€í•™êµì— ëŒ€í•œ ë‹¤ì–‘í•œ ì •ë³´ë“¤ì„ ì œê³µí•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.
                    All answers are based on the introduce.txt file.
                    Please introduce yourself when the questioner greets you.
                    Please introduce yourself when the questioner says "Hi", "Hello", "ì•ˆë…•".
                    ë„ˆëŠ” í•™ê³¼, ì¥í•™ê¸ˆ, ë“±ë¡, ì„±ì , ì¡¸ì—…, ìˆ˜ê°•ì‹ ì²­, ì…”í‹€ë²„ìŠ¤, êµí†µ, ì‹œì„¤ì •ë³´, í•™ì‚¬ì¼ì •, ë„ì„œê´€, í•™êµ ê±´ë¬¼, ì¦ëª…ì„œ, í›„ë¬¸ ì •ë³´, ë™ì•„ë¦¬ ë“± ë‹¤ì–‘í•œ ì£¼ì œì˜ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
                    The database consists of detailed information in each category's txt file.
                    Your answers should be delivered in an accurate, informative, and friendly dialogue style.
                    They should also be written in bullet style format.
                    URLs to various homepages must be spaced one space at the end.
                    When you tell me the URL, don't skip it and tell me the whole thing.
                    Don't make up anything that's not relevant to what you asked.
                    Please ensure the information provided is up to date and relevant to the user's query and files.
                    You always refers to factual statements that can be referenced.
                    You says only facts related to ì‚¼ìœ¡ëŒ€í•™êµ and does not add information on its own.:
        {context}
        
        Question: {question}
        """
        prompt = ChatPromptTemplate.from_template(template)
        llm = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0, max_tokens=2048)
        chain = RunnableMap({
            "context": lambda x: retriever.get_relevant_documents(x['question']),
            "question": lambda x: x['question']
        }) | prompt | llm
        response = chain.invoke({'question': user_input}).content
        return response
    except Exception as e:
        logging.error(f"An error occurred: {str(e)}")
        return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def main():
    st.set_page_config(
        page_title="SYU-GPT",
        # page_icon="ğŸ˜ƒ",
        page_icon="photo/Logo.png",
        layout="wide",
        initial_sidebar_state="auto",
        menu_items={
            'Get Help': 'https://www.extremelycoolapp.com/help',
            'Report a bug': "https://www.extremelycoolapp.com/bug",
        }
    )
    setup_environment()
    st.title('SYU-GPT', anchor=False)
    # ë¨¼ì €, subheaderì™€ captionì„ í¬í•¨í•˜ëŠ” ë¶€ë¶„ì„ st.empty()ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹ˆ í™€ë”ë¡œ ë§Œë“­ë‹ˆë‹¤.
    info_placeholder = st.empty()
    # ì´ì œ info_placeholderë¥¼ ì‚¬ìš©í•˜ì—¬ subheaderì™€ captionì„ í‘œì‹œí•©ë‹ˆë‹¤.
    with info_placeholder.container():
        st.subheader('ì‚¼ìœ¡ëŒ€í•™êµ ê²€ìƒ‰ ì—”ì§„', anchor=False)
        st.caption('ì—¬ëŸ¬ë¶„ì´ ê²€ìƒ‰í•˜ê³  ì‹¶ì€ í•™êµ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì„¸ìš”!')
        st.caption('ë°ì´í„°ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ì¤‘ì…ë‹ˆë‹¤.')
        st.caption('ì‚¼ìœ¡ëŒ€í•™êµ ì¬í•™ìƒì´ë¼ë©´ ì‚¬ìš©í•´ë³´ì„¸ìš”! ğŸ˜Š')
        st.caption(' ')
        st.caption('ì‚¬ìš©í•˜ì‹œëŠ”ë° ë¶ˆí¸í•œ ì ì´ ìˆìœ¼ë©´ ì•„ë˜ ì‚¬ìš© ê°€ì´ë“œë¥¼ ì°¸ê³ í•´ë³´ì„¸ìš”!')
        st.caption(' ')
        st.page_link("pages/guide.py", label="ì‚¬ìš© ê°€ì´ë“œ ë°”ë¡œê°€ê¸°", help="ì‚¬ìš© ê°€ì´ë“œë¡œ ì´ë™í•©ë‹ˆë‹¤.", icon="â–¶")
        st.caption(' ')
        st.markdown('**ì•ˆë…•! ì´ë¼ê³  ì¸ì‚¬í•´ë³´ì„¸ìš” âœ‹âœ‹**')
    # ì‚¬ì´ë“œë°”
    st.sidebar.image("photo/syugptLogo.png")
    hide_img_fs = '''
    <style>
    button[title="View fullscreen"]{
        visibility: hidden;}
    </style>
    '''
    st.sidebar.markdown(hide_img_fs, unsafe_allow_html=True)
    st.sidebar.write('-' * 50)
    st.sidebar.subheader("Menu")
    st.sidebar.page_link("main.py", label="Home", help="í™ˆ í™”ë©´ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤", icon="ğŸ ")
    st.sidebar.page_link("pages/greeting.py", label="Greeting", icon="âœ‹")
    st.sidebar.page_link("pages/guide.py", label="User's Guide", icon="â“")
    st.sidebar.subheader("Other Web")
    st.sidebar.page_link("https://www.syu.ac.kr/", label="Sahmyook University", help="ì‚¼ìœ¡ëŒ€í•™êµ ê³µì‹ ì‚¬ì´íŠ¸ë¡œ ì´ë™í•©ë‹ˆë‹¤")
    st.sidebar.page_link("https://chat.openai.com/", label="ChatGPT", help="Chat GPT ì‚¬ì´íŠ¸ë¡œ ì´ë™í•©ë‹ˆë‹¤")
    st.sidebar.page_link("https://gabean.kr/", label="GaBean", help="ê°œë°œìì˜ ë˜ ë‹¤ë¥¸ ì›¹ ì‚¬ì´íŠ¸ë¡œ ì´ë™í•©ë‹ˆë‹¤")
    if "chat_session" not in st.session_state:
        st.session_state.messages = []
    if user_input := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”."):
        info_placeholder.empty()
        try:
            with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                response = generate_response(user_input)
            if response.startswith("ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:"):
                st.error(response)
            else:
                with st.chat_message("user", avatar="ğŸ§ƒ"):
                    st.markdown(user_input)
                st.session_state.messages.append({"role": "user", "content": user_input})
                with st.chat_message("SYU-GPT", avatar="photo/Logo.png"):
                    st.markdown(response)
                    st.caption(' ')
                    st.caption('ì‚¬ìš©í•˜ì‹œëŠ”ë° ë¶ˆí¸í•œ ì ì´ ìˆìœ¼ë©´ ì•„ë˜ ì‚¬ìš© ê°€ì´ë“œë¥¼ ì°¸ê³ í•´ë³´ì„¸ìš”!')
                    st.caption(' ')
                    st.page_link("pages/guide.py", label="ì‚¬ìš© ê°€ì´ë“œ ë°”ë¡œê°€ê¸°", help="ì‚¬ìš© ê°€ì´ë“œë¡œ ì´ë™í•©ë‹ˆë‹¤.", icon="â–¶")
                st.session_state.messages.append({"role": "SYU-GPT", "content": response})
        except Exception as e:
            st.error("ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {}".format(e))
if __name__ == "__main__":
    main()
